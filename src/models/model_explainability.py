import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
from sklearn.inspection import permutation_importance

class ModelExplainability:
    def __init__(self):
        """Initialize model explainability"""
        self.shap_values = {}
        self.feature_importance = {}
        
    def load_data(self):
        """Load test data"""
        self.test_df = pd.read_csv(r'F:\Projects\insurance_claim_prediction\data\test_engineered.csv')
        X_test = self.test_df.drop(['claim_severity', 'policy_id'], axis=1)
        y_test = np.log1p(self.test_df['claim_severity'])
        return X_test, y_test
    
    def load_model(self, model_path):
        """Load trained model"""
        return joblib.load(model_path)
    
    def get_feature_importance(self, model, X_test):
        """Get feature importance for tree-based models"""
        preprocessor = model.named_steps['preprocessor']
        cat_features_out = preprocessor.named_transformers_['cat'].get_feature_names_out()

        # These are the *original* numerical columns that went into the polynomial features
        # The final feature names are generated by the PolynomialFeatures transformer
        numerical_cols = [
            'driver_age^2', 'driver_age vehicle_age', 'driver_age past_claims',
            'vehicle_age^2', 'vehicle_age past_claims', 'past_claims^2',
            'driver_age', 'vehicle_age', 'past_claims'
        ]

        # Combine feature names in the correct order
        feature_names = list(cat_features_out) + numerical_cols
        importances = model.named_steps['model'].feature_importances_

        if len(feature_names) != len(importances):
            raise ValueError(f"Mismatch in feature names ({len(feature_names)}) and importances ({len(importances)})")

        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values(by='importance', ascending=False)

        return feature_importance_df
    
    def create_shap_explainer(self, model, X_test):
        """Create and return a SHAP explainer object."""
        preprocessor = model.named_steps['preprocessor']
        X_transformed = preprocessor.transform(X_test)
        
        # Use KernelExplainer for models with preprocessing pipelines
        # We use a sample of the test data as the background dataset for the explainer
        # The explainer needs the final model's prediction function, not the whole pipeline
        explainer = shap.KernelExplainer(model.named_steps['model'].predict, shap.sample(X_transformed, 50))
        return explainer
    
    def calculate_shap_values(self, model, X_test, model_name):
        """Calculate SHAP values"""
        # Get preprocessor and model
        preprocessor = model.named_steps['preprocessor']
        model = model.named_steps['model']
        
        # Transform data
        X_transformed = preprocessor.transform(X_test)
        
        # Convert to numpy array for SHAP
        X_array = X_transformed.toarray() if hasattr(X_transformed, 'toarray') else X_transformed
        
        # Create SHAP Explainer
        explainer = shap.Explainer(model)
        
        # Calculate SHAP values
        shap_values = explainer(X_array)
        
        # Get feature names
        cat_features_out = preprocessor.named_transformers_['cat'].get_feature_names_out()
        numerical_cols = [
            'driver_age^2', 'driver_age vehicle_age', 'driver_age past_claims',
            'vehicle_age^2', 'vehicle_age past_claims', 'past_claims^2',
            'driver_age', 'vehicle_age', 'past_claims'
        ]
        feature_names = list(cat_features_out) + numerical_cols
        
        # Store results
        self.shap_values[model_name] = {
            'explainer': explainer,
            'shap_values': shap_values,
            'X_test': X_test,
            'X_transformed': X_transformed,
            'feature_names': feature_names
        }
        
        return shap_values
    
    def plot_feature_importance(self, importance_df, model_name):
        """Plot feature importance"""
        plt.figure(figsize=(10, 8))
        sns.barplot(
            x='importance',
            y='feature',
            data=importance_df.head(15)
        )
        plt.title(f'Top 15 Feature Importance for {model_name}')
        plt.tight_layout()
        plt.savefig(f'reports/figures/{model_name}_feature_importance.png')
        plt.close()
    
    def plot_shap_summary(self, model_name):
        """Plot SHAP summary plot"""
        shap_values = self.shap_values[model_name]['shap_values']
        feature_names = self.shap_values[model_name]['feature_names']
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(
            shap_values,
            feature_names,
            plot_type='bar',
            max_display=15,
            show=False
        )
        plt.title(f'SHAP Feature Importance for {model_name}')
        plt.tight_layout()
        plt.savefig(f'reports/figures/{model_name}_shap_summary.png')
        plt.close()
    
    def plot_shap_waterfall(self, model_name, instance_idx=0):
        """Plot SHAP waterfall plot for a single instance"""
        shap_values = self.shap_values[model_name]['shap_values']
        X_transformed = self.shap_values[model_name]['X_transformed']
        
        plt.figure(figsize=(12, 8))
        shap.plots.waterfall(
            shap_values[instance_idx],
            max_display=15,
            show=False
        )
        plt.title(f'SHAP Waterfall Plot for {model_name} - Instance {instance_idx}')
        plt.tight_layout()
        plt.savefig(f'reports/figures/{model_name}_shap_waterfall_{instance_idx}.png')
        plt.close()
    
    def analyze_model(self, model_path, model_name):
        """Analyze a single model"""
        print(f"\nAnalyzing {model_name}...")
        
        # Load model and data
        model = self.load_model(model_path)
        X_test, y_test = self.load_data()

        # Create and save the SHAP explainer for the best model
        explainer = self.create_shap_explainer(model, X_test)
        joblib.dump(explainer, r'F:\Projects\insurance_claim_prediction\models\shap_explainer.pkl')
        print("SHAP explainer for best model has been saved.")

        # Get feature importance
        importance_df = self.get_feature_importance(model, X_test)
        self.feature_importance[model_name] = importance_df
        
        # Plot feature importance
        self.plot_feature_importance(importance_df, model_name)
        
        # Calculate and plot SHAP values
        shap_values = self.calculate_shap_values(model, X_test, model_name)
        self.plot_shap_summary(model_name)
        self.plot_shap_waterfall(model_name)
        
        print(f"\nTop 5 features affecting {model_name} predictions:")
        print(importance_df.head(5))
        
        # Generate business insights
        self.generate_business_insights(importance_df, model_name)
    
    def generate_business_insights(self, importance_df, model_name):
        """Generate business insights from feature importance"""
        print(f"\nBusiness Insights for {model_name}:")
        
        # Get top features
        top_features = importance_df.head(5)['feature'].tolist()
        
        # Map features to business insights
        insights = {
            'driver_age': 'Driver age significantly impacts claim costs',
            'vehicle_age': 'Vehicle age is a key risk factor',
            'past_claims': 'Past claims are an important predictor of future claims',
            'driver_age vehicle_age': 'The interaction between driver age and vehicle age affects claim severity',
            'driver_age past_claims': 'The interaction between driver age and past claims impacts claim costs'
        }
        
        print("\nKey Risk Factors:")
        for feature in top_features:
            if feature in insights:
                print(f"- {feature}: {insights[feature]}")
        
        # Generate more specific insights based on feature importance
        print("\nDetailed Business Insights:")
        
        # Analyze driver age feature
        if 'driver_age' in top_features:
            print("- Driver age is a significant predictor of claim costs")
            print("  * Younger drivers may have higher risk profiles")
            print("  * Older drivers may have lower risk profiles")
        
        # Analyze vehicle age feature
        if 'vehicle_age' in top_features:
            print("- Vehicle age is an important risk factor")
            print("  * Newer vehicles may have higher repair costs")
            print("  * Older vehicles may have lower repair costs")
        
        # Analyze past claims feature
        if 'past_claims' in top_features:
            print("- Past claims are an important predictor of future claims")
            print("  * Drivers with a history of claims may have higher risk profiles")
            print("  * Drivers with no past claims may have lower risk profiles")
        
        # Analyze interaction between driver age and vehicle age
        if 'driver_age vehicle_age' in top_features:
            print("- The interaction between driver age and vehicle age affects claim severity")
            print("  * Younger drivers with newer vehicles may have higher risk profiles")
            print("  * Older drivers with older vehicles may have lower risk profiles")
        
        # Analyze interaction between driver age and past claims
        if 'driver_age past_claims' in top_features:
            print("- The interaction between driver age and past claims impacts claim costs")
            print("  * Younger drivers with a history of claims may have higher risk profiles")
            print("  * Older drivers with no past claims may have lower risk profiles")
    
    def analyze_all_models(self):
        """Analyze the best model and save the SHAP explainer."""
        self.analyze_model('models/best_model.pkl', 'XGBoost')
        
        print("\nAnalysis complete!")

def main():
    """Main function to analyze all models"""
    analyzer = ModelExplainability()
    analyzer.analyze_all_models()

if __name__ == "__main__":
    main()
