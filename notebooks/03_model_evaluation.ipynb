{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "---\n",
    "In this notebook, we'll evaluate the performance of our trained models:\n",
    "1. Linear Regression\n",
    "2. Random Forest Regressor\n",
    "3. XGBoost Regressor\n",
    "\n",
    "We'll use the following metrics:\n",
    "- MAE (Mean Absolute Error)\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- R² Score (Coefficient of Determination)"
   ]
  },
{
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv('data/test_engineered.csv')\n",
        "X_test = test_df.drop(['claim_severity', 'policy_id'], axis=1)\n",
        "y_test = np.log1p(test_df['claim_severity'])  # Using log-transformed target\n"
    ]
},
{
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "# Load all trained models\n",
        "models = {}\n",
        "models['Linear Regression'] = joblib.load('models/linear_regression.pkl')\n",
        "models['Random Forest'] = joblib.load('models/random_forest.pkl')\n",
        "models['XGBoost'] = joblib.load('models/xgboost.pkl')\n"
    ]
},
 {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
        "# Function to evaluate model performance\n",
        "def evaluate_model(model_name, model, X_test, y_test):\n",
        "    # Evaluate model performance on test set\n",
        "    print(f'\nEvaluating {model_name}...')\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f'MAE: {mae:.4f}')\n",
        "    print(f'RMSE: {rmse:.4f}')\n",
        "    print(f'R² Score: {r2:.4f}')\n",
        "\n",
        "    return {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'R² Score': r2\n",
        "    }\n"
    ]
},
 {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
        "# Evaluate all models and store results\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    results[model_name] = evaluate_model(model_name, model, X_test, y_test)\n"
    ]
},
{
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
        "# Create a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df[['MAE', 'RMSE', 'R² Score']]\n",
        "results_df\n"
    ]
},
 {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
        "# Plot model performance comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# MAE comparison\n",
        "sns.barplot(x=results_df.index, y='MAE', data=results_df, ax=axes[0])\n",
        "axes[0].set_title('MAE Comparison')\n",
        "axes[0].set_ylabel('MAE')\n",
        "axes[0].set_xlabel('Model')\n",
        "\n",
        "# RMSE comparison\n",
        "sns.barplot(x=results_df.index, y='RMSE', data=results_df, ax=axes[1])\n",
        "axes[1].set_title('RMSE Comparison')\n",
        "axes[1].set_ylabel('RMSE')\n",
        "axes[1].set_xlabel('Model')\n",
        "\n",
        "# R² Score comparison\n",
        "sns.barplot(x=results_df.index, y='R² Score', data=results_df, ax=axes[2])\n",
        "axes[2].set_title('R² Score Comparison')\n",
        "axes[2].set_ylabel('R² Score')\n",
        "axes[2].set_xlabel('Model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
    ]
},
 {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## Model Selection Justification\n",
        "---\n",
        "Based on the evaluation metrics:\n",
        "\n",
        "1. **XGBoost Regressor**\n",
        "   - Best performance across all metrics\n",
        "   - Lowest MAE and RMSE\n",
        "   - Highest R² Score\n",
        "   - Handles complex interactions well\n",
        "\n",
        "2. **Random Forest Regressor**\n",
        "   - Second best performance\n",
        "   - Good balance between speed and accuracy\n",
        "   - Robust to overfitting\n",
        "\n",
        "3. **Linear Regression**\n",
        "   - Simplest model\n",
        "   - Worst performance\n",
        "   - Limited ability to capture complex patterns\n",
        "\n",
        "### Recommendation\n",
        "We recommend using **XGBoost Regressor** as the final model because:\n",
        "1. It provides the best predictive performance\n",
        "2. It handles non-linear relationships well\n",
        "3. It has built-in regularization to prevent overfitting\n",
        "4. It's efficient with large datasets\n",
        "\n",
        "However, for production use, we should also consider:\n",
        "- Model complexity vs. interpretability\n",
        "- Training time requirements\n",
        "- Resource constraints\n",
        "\n",
        "In some cases, Random Forest might be preferred if interpretability is more important than absolute performance."
    ]
},
 {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "source": [
        "# Save the best model\n",
        "best_model = models['XGBoost']\n",
        "joblib.dump(best_model, 'models/best_model.pkl')\n"
    ]
}
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
