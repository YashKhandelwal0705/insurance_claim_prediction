{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "---\n",
    "In this notebook, we'll evaluate the performance of our trained models:\n",
    "1. Linear Regression\n",
    "2. Random Forest Regressor\n",
    "3. XGBoost Regressor\n",
    "\n",
    "We'll use the following metrics:\n",
    "- MAE (Mean Absolute Error)\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- R² Score (Coefficient of Determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd
",
    "import numpy as np
",
    "import matplotlib.pyplot as plt
",
    "import seaborn as sns
",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
",
    "import joblib
",
    "import os
",
    "\n",
    "# Set style for plots
",
    "plt.style.use('seaborn')
",
    "sns.set_palette('husl')
",
    "\n",
    "# Load test data
",
    "test_df = pd.read_csv('data/test_engineered.csv')
",
    "X_test = test_df.drop(['claim_severity', 'policy_id'], axis=1)
",
    "y_test = np.log1p(test_df['claim_severity'])  # Using log-transformed target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load all trained models
",
    "models = {}
",
    "models['Linear Regression'] = joblib.load('models/linear_regression.pkl')
",
    "models['Random Forest'] = joblib.load('models/random_forest.pkl')
",
    "models['XGBoost'] = joblib.load('models/xgboost.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to evaluate model performance
",
    "def evaluate_model(model_name, model, X_test, y_test):
",
    "    """Evaluate model performance on test set"""
",
    "    print(f'\nEvaluating {model_name}...')
",
    "    
",
    "    # Predict
",
    "    y_pred = model.predict(X_test)
",
    "    
",
    "    # Calculate metrics
",
    "    mae = mean_absolute_error(y_test, y_pred)
",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
",
    "    r2 = r2_score(y_test, y_pred)
",
    "    
",
    "    # Print results
",
    "    print(f'MAE: {mae:.4f}')
",
    "    print(f'RMSE: {rmse:.4f}')
",
    "    print(f'R² Score: {r2:.4f}')
",
    "    
",
    "    return {\n",
    "        'MAE': mae,
",
    "        'RMSE': rmse,
",
    "        'R² Score': r2
",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate all models and store results
",
    "results = {}
",
    "for model_name, model in models.items():
",
    "    results[model_name] = evaluate_model(model_name, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a DataFrame for better visualization
",
    "results_df = pd.DataFrame(results).T
",
    "results_df = results_df[['MAE', 'RMSE', 'R² Score']]
",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot model performance comparison
",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))
",
    "\n",
    "# MAE comparison
",
    "sns.barplot(x=results_df.index, y='MAE', data=results_df, ax=axes[0])
",
    "axes[0].set_title('MAE Comparison')
",
    "axes[0].set_ylabel('MAE')
",
    "axes[0].set_xlabel('Model')
",
    "\n",
    "# RMSE comparison
",
    "sns.barplot(x=results_df.index, y='RMSE', data=results_df, ax=axes[1])
",
    "axes[1].set_title('RMSE Comparison')
",
    "axes[1].set_ylabel('RMSE')
",
    "axes[1].set_xlabel('Model')
",
    "\n",
    "# R² Score comparison
",
    "sns.barplot(x=results_df.index, y='R² Score', data=results_df, ax=axes[2])
",
    "axes[2].set_title('R² Score Comparison')
",
    "axes[2].set_ylabel('R² Score')
",
    "axes[2].set_xlabel('Model')
",
    "\n",
    "plt.tight_layout()
",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Justification\n",
    "---\n",
    "Based on the evaluation metrics:\n",
    "\n",
    "1. **XGBoost Regressor**\n",
    "   - Best performance across all metrics\n",
    "   - Lowest MAE and RMSE\n",
    "   - Highest R² Score\n",
    "   - Handles complex interactions well\n",
    "\n",
    "2. **Random Forest Regressor**\n",
    "   - Second best performance\n",
    "   - Good balance between speed and accuracy\n",
    "   - Robust to overfitting\n",
    "\n",
    "3. **Linear Regression**\n",
    "   - Simplest model\n",
    "   - Worst performance\n",
    "   - Limited ability to capture complex patterns\n",
    "\n",
    "### Recommendation\n",
    "We recommend using **XGBoost Regressor** as the final model because:\n",
    "1. It provides the best predictive performance\n",
    "2. It handles non-linear relationships well\n",
    "3. It has built-in regularization to prevent overfitting\n",
    "4. It's efficient with large datasets\n",
    "\n",
    "However, for production use, we should also consider:\n",
    "- Model complexity vs. interpretability\n",
    "- Training time requirements\n",
    "- Resource constraints\n",
    "\n",
    "In some cases, Random Forest might be preferred if interpretability is more important than absolute performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the best model
",
    "best_model = models['XGBoost']
",
    "joblib.dump(best_model, 'models/best_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
